"""
Multi-Modal Malware Detector
Ensemble of CNN (bytes) + RNN (imports) + GNN (call graph)
"""

import torch
import torch.nn as nn
import pytorch_lightning as pl
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, Batch
import torchvision.models as models
from typing import Dict, List, Tuple


class CNNBytesClassifier(nn.Module):
    """
    CNN branch for raw binary bytes analysis
    Uses ResNet18 adapted for 1D byte sequences
    """
    
    def __init__(self, num_classes: int = 50, chunk_size: int = 1024):
        super().__init__()
        
        # Use ResNet18 as backbone (modify for 1 channel)
        self.backbone = models.resnet18(pretrained=True)
        self.backbone.conv1 = nn.Conv2d(
            1, 64, kernel_size=7, stride=2, padding=3, bias=False
        )
        
        # Replace final FC layer
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(num_features, num_classes)
        
    def forward(self, byte_chunks):
        """
        Args:
            byte_chunks: [batch, 1, 32, 32] - reshaped byte chunks
        """
        return self.backbone(byte_chunks)


class RNNImportClassifier(nn.Module):
    """
    RNN branch for import table sequence analysis
    Uses LSTM to model import dependencies
    """
    
    def __init__(
        self,
        vocab_size: int = 10000,
        embedding_dim: int = 128,
        hidden_dim: int = 256,
        num_layers: int = 2,
        num_classes: int = 50
    ):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
    def forward(self, import_sequences):
        """
        Args:
            import_sequences: [batch, seq_len] - tokenized import names
        """
        # Embed
        embedded = self.embedding(import_sequences)  # [batch, seq_len, emb_dim]
        
        # LSTM
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # Use final hidden states from both directions
        final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)  # [batch, hidden*2]
        
        # Classify
        logits = self.classifier(final_hidden)
        return logits


class GNNCallGraphClassifier(nn.Module):
    """
    GNN branch for function call graph analysis
    Uses Graph Convolutional Networks
    """
    
    def __init__(
        self,
        in_channels: int = 128,
        hidden_channels: int = 256,
        num_classes: int = 50,
        num_layers: int = 3
    ):
        super().__init__()
        
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(in_channels, hidden_channels))
        
        for _ in range(num_layers - 1):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))
        
        self.classifier = nn.Sequential(
            nn.Linear(hidden_channels, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
    def forward(self, graph_batch):
        """
        Args:
            graph_batch: PyG Batch object with x, edge_index, batch
        """
        x = graph_batch.x
        edge_index = graph_batch.edge_index
        batch = graph_batch.batch
        
        # Graph convolutions
        for conv in self.convs:
            x = conv(x, edge_index)
            x = torch.relu(x)
        
        # Global pooling
        x = global_mean_pool(x, batch)  # [batch, hidden_channels]
        
        # Classify
        logits = self.classifier(x)
        return logits


class MalwareEnsemble(pl.LightningModule):
    """
    Multi-modal ensemble for malware family classification
    Combines CNN + RNN + GNN with weighted fusion
    """
    
    def __init__(
        self,
        num_classes: int = 50,
        cnn_weight: float = 0.3,
        rnn_weight: float = 0.3,
        gnn_weight: float = 0.4,
        learning_rate: float = 1e-3
    ):
        super().__init__()
        self.save_hyperparameters()
        
        # Three branches
        self.cnn_branch = CNNBytesClassifier(num_classes)
        self.rnn_branch = RNNImportClassifier(num_classes=num_classes)
        self.gnn_branch = GNNCallGraphClassifier(num_classes=num_classes)
        
        # Fusion weights
        self.register_buffer('weights', torch.tensor([cnn_weight, rnn_weight, gnn_weight]))
        
        # Meta-learner (stacking)
        self.meta_learner = nn.Sequential(
            nn.Linear(num_classes * 3, 512),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
        self.criterion = nn.CrossEntropyLoss()
        self.learning_rate = learning_rate
        
    def forward(
        self,
        byte_chunks,
        import_sequences,
        call_graphs,
        use_meta: bool = True
    ):
        """
        Args:
            byte_chunks: [batch, 1, 32, 32]
            import_sequences: [batch, seq_len]
            call_graphs: PyG Batch
            use_meta: Whether to use meta-learner (True) or weighted averaging (False)
        """
        # Get predictions from each branch
        cnn_logits = self.cnn_branch(byte_chunks)
        rnn_logits = self.rnn_branch(import_sequences)
        gnn_logits = self.gnn_branch(call_graphs)
        
        if use_meta:
            # Stacking - concatenate and use meta-learner
            stacked = torch.cat([cnn_logits, rnn_logits, gnn_logits], dim=1)
            final_logits = self.meta_learner(stacked)
        else:
            # Weighted averaging
            final_logits = (
                self.weights[0] * cnn_logits +
                self.weights[1] * rnn_logits +
                self.weights[2] * gnn_logits
            )
        
        return final_logits, (cnn_logits, rnn_logits, gnn_logits)
    
    def training_step(self, batch, batch_idx):
        byte_chunks, import_seqs, call_graphs, labels = batch
        
        final_logits, branch_logits = self(
            byte_chunks,
            import_seqs,
            call_graphs,
            use_meta=True
        )
        
        # Main loss
        loss = self.criterion(final_logits, labels)
        
        # Auxiliary losses for each branch
        cnn_logits, rnn_logits, gnn_logits = branch_logits
        aux_loss = (
            self.criterion(cnn_logits, labels) +
            self.criterion(rnn_logits, labels) +
            self.criterion(gnn_logits, labels)
        ) / 3.0
        
        total_loss = loss + 0.3 * aux_loss
        
        # Metrics
        preds = torch.argmax(final_logits, dim=1)
        acc = (preds == labels).float().mean()
        
        self.log('train_loss', total_loss, prog_bar=True)
        self.log('train_acc', acc, prog_bar=True)
        self.log('aux_loss', aux_loss)
        
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        byte_chunks, import_seqs, call_graphs, labels = batch
        
        final_logits, _ = self(
            byte_chunks,
            import_seqs,
            call_graphs,
            use_meta=True
        )
        
        loss = self.criterion(final_logits, labels)
        
        preds = torch.argmax(final_logits, dim=1)
        acc = (preds == labels).float().mean()
        
        # Calculate per-class metrics
        probs = torch.softmax(final_logits, dim=1)
        
        self.log('val_loss', loss, prog_bar=True)
        self.log('val_acc', acc, prog_bar=True)
        
        return {
            'preds': preds,
            'labels': labels,
            'probs': probs
        }
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.learning_rate,
            weight_decay=0.01
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': scheduler,
            'monitor': 'val_loss'
        }


if __name__ == "__main__":
    # Test ensemble instantiation
    ensemble = MalwareEnsemble(num_classes=50)
    print(f"Ensemble parameters: {sum(p.numel() for p in ensemble.parameters()):,}")
    
    # Test forward pass with dummy data
    batch_size = 4
    byte_chunks = torch.randn(batch_size, 1, 32, 32)
    import_seqs = torch.randint(0, 1000, (batch_size, 100))
    
    # Create dummy graph
    edge_index = torch.tensor([[0, 1, 2], [1, 2, 0]], dtype=torch.long)
    x = torch.randn(3, 128)
    graph = Data(x=x, edge_index=edge_index)
    graph_batch = Batch.from_data_list([graph] * batch_size)
    
    logits, _ = ensemble(byte_chunks, import_seqs, graph_batch)
    print(f"Output shape: {logits.shape}")
